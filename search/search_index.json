{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aviary - Study stochastic parrots in the wild","text":"<p>Go on bird watch right now: \ud83e\udd9c\ud83d\udd0d Aviary \ud83e\udd9c\ud83d\udd0d</p> <p></p> <p>Aviary is an app that lets you interact with a variety of  large language models (LLMs) in a single place.  You can compare the outputs of different models directly, rank them by quality, get a cost and latency estimate, and more. In particular, it offers good support for  Transformer models hosted on Hugging Face and in many cases also  supports DeepSpeed inference acceleration. </p> <p>Aviary is built on top of Ray by Anyscale. It's an open source project, which means that you can deploy it yourself to a cloud service,  or simply use our hosted version. If you would like to use a managed version of Aviary specific to your company, please reach out to us.</p>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#using-the-aviary-cli","title":"Using the Aviary CLI","text":"<p>Aviary comes with a CLI that allows you to interact with the backend directly, without using the Gradio frontend. Installing Aviary as described earlier will install the <code>aviary</code> CLI as well. You can get a list of all available commands by running <code>aviary --help</code>.</p> <p>Currently, <code>aviary</code> supports a few basic commands, all of which can be used with the <code>--help</code> flag to get more information:</p> <pre><code># Get a list of all available models in Aviary\naviary models\n\n# Query a model with a list of prompts\naviary query --model &lt;model-name&gt; --prompt &lt;prompt_1&gt; --prompt &lt;prompt_2&gt;\n\n# Run a query on a text file of prompts\naviary query  --model &lt;model-name&gt; --prompt-file &lt;prompt-file&gt;\n\n# Evaluate the quality of responses with GPT-4 for evaluation\naviary evaluate --input-file &lt;query-result-file&gt;\n\n# Start a new model in Aviary from provided configuration\naviary run &lt;model&gt;\n</code></pre>"},{"location":"cli/#cli-examples","title":"CLI examples","text":""},{"location":"cli/#listing-all-available-models","title":"Listing all available models","text":"<pre><code>aviary models\n</code></pre> <pre><code>mosaicml/mpt-7b-instruct\nCarperAI/stable-vicuna-13b-delta\ndatabricks/dolly-v2-12b\nRWKV/rwkv-raven-14b\nmosaicml/mpt-7b-chat\nstabilityai/stablelm-tuned-alpha-7b\nlmsys/vicuna-13b-delta-v1.1\nmosaicml/mpt-7b-storywriter\nh2oai/h2ogpt-oasst1-512-12b\nOpenAssistant/oasst-sft-7-llama-30b-xor\n</code></pre>"},{"location":"cli/#running-two-models-on-the-same-prompt","title":"Running two models on the same prompt","text":"<pre><code>aviary query --model mosaicml/mpt-7b-instruct --model RWKV/rwkv-raven-14b \\\n  --prompt \"what is love?\"\n</code></pre> <pre><code>mosaicml/mpt-7b-instruct:\nlove can be defined as feeling of affection, attraction or ...\nRWKV/rwkv-raven-14b:\nLove is a feeling of strong affection and care for someone or something...\n</code></pre>"},{"location":"cli/#running-a-batch-query-of-two-prompts-on-the-same-model","title":"Running a batch-query of two prompts on the same model","text":"<pre><code>aviary query --model mosaicml/mpt-7b-instruct \\\n  --prompt \"what is love?\" --prompt \"why are we here?\"\n</code></pre>"},{"location":"cli/#running-a-query-on-a-text-file-of-prompts","title":"Running a query on a text file of prompts","text":"<pre><code>aviary query --model mosaicml/mpt-7b-instruct --prompt-file prompts.txt\n</code></pre>"},{"location":"cli/#evaluating-the-quality-of-responses-with-gpt-4-for-evaluation","title":"Evaluating the quality of responses with GPT-4 for evaluation","text":"<pre><code> aviary evaluate --input-file aviary-output.json --evaluator gpt-4\n</code></pre> <p>This will result in a leaderboard-like ranking of responses, but also save the results to file:</p> <pre><code>What is the best indie band of the 90s?\n                                              Evaluation results (higher ranks are better)                                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                    \u2503 Rank \u2503                                                                                            Response \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 mosaicml/mpt-7b-instruct \u2502 1    \u2502  The Shins are often considered to be one of the greatest bands from this era, with their album 'Oh \u2502\n\u2502                          \u2502      \u2502        Inverted World' being widely regarded as one of the most influential albums in recent memory \u2502\n\u2502 RWKV/rwkv-raven-14b      \u2502 2    \u2502 It's subjective and depends on personal taste. Some people might argue that Nirvana or The Smashing \u2502\n\u2502                          \u2502      \u2502                       Pumpkins were the best, while others might prefer Sonic Youth or Dinosaur Jr. \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can also use the Gradio API directly, by following the instructions provided in the Aviary documentation.</p>"},{"location":"kuberay/deploy-on-eks/","title":"Deploy Aviary on Amazon EKS using KubeRay","text":"<ul> <li>Note that this document will be extended to include Ray autoscaling and the deployment of multiple models in the near future.</li> </ul>"},{"location":"kuberay/deploy-on-eks/#step-1-create-a-kubernetes-cluster-on-amazon-eks","title":"Step 1: Create a Kubernetes cluster on Amazon EKS","text":"<p>Follow the first two steps in this AWS documentation to: (1) Create your Amazon EKS cluster (2) Configure your computer to communicate with your cluster.</p>"},{"location":"kuberay/deploy-on-eks/#step-2-create-node-groups-for-the-amazon-eks-cluster","title":"Step 2: Create node groups for the Amazon EKS cluster","text":"<p>You can follow \"Step 3: Create nodes\" in this AWS documentation to create node groups. The following section provides more detailed information.</p>"},{"location":"kuberay/deploy-on-eks/#create-a-cpu-node-group","title":"Create a CPU node group","text":"<p>Create a CPU node group for all Pods except Ray GPU workers, such as KubeRay operator, Ray head, and CoreDNS Pods.</p> <ul> <li>Create a CPU node group</li> <li>Instance type: m5.xlarge (4 vCPU; 16 GB RAM)</li> <li>Disk size: 256 GB</li> <li>Desired size: 1, Min size: 0, Max size: 1</li> </ul>"},{"location":"kuberay/deploy-on-eks/#create-a-gpu-node-group","title":"Create a GPU node group","text":"<p>Create a GPU node group for Ray GPU workers.</p> <ul> <li>Create a GPU node group</li> <li>Add a Kubernetes taint to prevent CPU Pods from being scheduled on this GPU node group<ul> <li>Key: ray.io/node-type, Value: worker, Effect: NoSchedule</li> </ul> </li> <li>AMI type: Bottlerocket NVIDIA (BOTTLEROCKET_x86_64_NVIDIA)</li> <li>Instance type: g5.12xlarge (4 GPU; 96 GB GPU Memory; 48 vCPUs; 192 GB RAM)</li> <li>Disk size: 1024 GB</li> <li>Desired size: 1, Min size: 0, Max size: 1</li> </ul> <p>Because this tutorial is for deploying 1 LLM, the maximum size of this GPU node group is 1. If you want to deploy multiple LLMs in this cluster, you may need to increase the value of the max size.</p> <p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p>"},{"location":"kuberay/deploy-on-eks/#step-3-verify-the-node-groups","title":"Step 3: Verify the node groups","text":"<p>If you encounter permission issues with <code>eksctl</code>, you can navigate to your AWS account's webpage and copy the credential environment variables, including <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>, from the \"Command line or programmatic access\" page.</p> <pre><code>eksctl get nodegroup --cluster ${YOUR_EKS_NAME}\n\n# CLUSTER         NODEGROUP       STATUS  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID                        ASG NAME                           TYPE\n# ${YOUR_EKS_NAME}     cpu-node-group  ACTIVE  2023-06-05T21:31:49Z    0               1               1                       m5.xlarge       AL2_x86_64                      eks-cpu-node-group-...     managed\n# ${YOUR_EKS_NAME}     gpu-node-group  ACTIVE  2023-06-05T22:01:44Z    0               1               1                       g5.12xlarge     BOTTLEROCKET_x86_64_NVIDIA      eks-gpu-node-group-...     managed\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#step-4-install-the-daemonset-for-nvidia-device-plugin-for-kubernetes","title":"Step 4: Install the DaemonSet for NVIDIA device plugin for Kubernetes","text":"<p>If you encounter permission issues with <code>kubectl</code>, you can follow \"Step 2: Configure your computer to communicate with your cluster\" in the AWS documentation.</p> <p>You can refer to the Amazon EKS optimized accelerated Amazon Linux AMIs or NVIDIA/k8s-device-plugin repository for more details.</p> <pre><code># Install the DaemonSet\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.9.0/nvidia-device-plugin.yml\n\n# Verify that your nodes have allocatable GPUs \nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ip-....us-west-2.compute.internal   4\n# ip-....us-west-2.compute.internal   &lt;none&gt;\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#step-5-install-a-kuberay-operator","title":"Step 5: Install a KubeRay operator","text":"<pre><code># Install both CRDs and KubeRay operator v0.5.0.\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0\n\n# It should be scheduled on the CPU node. If it is not, something is wrong.\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#step-6-create-a-raycluster-with-aviary","title":"Step 6: Create a RayCluster with Aviary","text":"<pre><code># path: deploy/kuberay\nkubectl apply -f kuberay.yaml\n</code></pre> <p>Something is worth noticing: * The <code>tolerations</code> for workers must match the taints on the GPU node group.     <code>yaml     # Please add the following taints to the GPU node.     tolerations:         - key: \"ray.io/node-type\"         operator: \"Equal\"         value: \"worker\"         effect: \"NoSchedule\"</code> * Update <code>rayStartParams.resources</code> for Ray scheduling. The <code>mosaicml--mpt-7b-chat.yaml</code> file uses both <code>accelerator_type_cpu</code> and <code>accelerator_type_a10</code>.     ```yaml     # Ray head: The Ray head has a Pod resource limit of 2 CPUs.     rayStartParams:       resources: '\"{\\\"accelerator_type_cpu\\\": 2}\"'</p> <pre><code># Ray workers: The Ray worker has a Pod resource limit of 48 CPUs and 4 GPUs.\nrayStartParams:\n    resources: '\"{\\\"accelerator_type_cpu\\\": 48, \\\"accelerator_type_a10\\\": 4}\"'\n```\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#step-7-deploy-a-llm-model-with-aviary","title":"Step 7: Deploy a LLM model with Aviary","text":"<pre><code># Step 7.1: Log in to the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- bash\n\n# Step 7.2: Deploy a `mosaicml/mpt-7b-chat` model\naviary run --model ./models/static_batching/mosaicml--mpt-7b-chat.yaml\n\n# Step 7.3: Check the Serve application status\nserve status\n\n# [Example output]\n# name: default\n# app_status:\n#   status: RUNNING\n#   message: ''\n#   deployment_timestamp: 1686006910.9571936\n# deployment_statuses:\n# - name: default_mosaicml--mpt-7b-chat\n#   status: HEALTHY\n#   message: ''\n# - name: default_RouterDeployment\n#   status: HEALTHY\n#   message: ''\n\n# Step 7.4: List all models\nexport AVIARY_URL=\"http://localhost:8000\"\naviary models\n\n# [Example output]\n# Connecting to Aviary backend at:  http://localhost:8000/\n# mosaicml/mpt-7b-chat\n\n# Step 7.5: Send a query to `mosaicml/mpt-7b-chat`.\naviary query --model mosaicml/mpt-7b-chat --prompt \"What are the top 5 most popular programming languages?\"\n\n# [Example output]\n# Connecting to Aviary backend at:  http://localhost:8000/\n# mosaicml/mpt-7b-chat:\n# 1. Python\n# 2. Java\n# 3. JavaScript\n# 4. C++\n# 5. C#\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#step-8-clean-up-resources","title":"Step 8: Clean up resources","text":"<p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p> <pre><code># Step 8.1: Delete the RayCluster\n# path: deploy/kuberay\nkubectl delete -f kuberay.yaml\n\n# Step 8.2: Uninstall the KubeRay operator chart\nhelm uninstall kuberay-operator\n\n# Step 8.3: Delete the Amazon EKS cluster via AWS Web UI\n</code></pre>"},{"location":"kuberay/deploy-on-gke/","title":"Deploy Aviary on Googke Kubernetes Engine (GKE) using KubeRay","text":"<p>In this tutorial, we will:</p> <ol> <li>Set up a Kubernetes cluster on GKE.</li> <li>Deploy the KubeRay operator and a Ray cluster on GKE.</li> <li> <p>Run an LLM model with Aviary.</p> </li> <li> <p>Note that this document will be extended to include Ray autoscaling and the deployment of multiple models in the near future.</p> </li> </ol>"},{"location":"kuberay/deploy-on-gke/#step-1-create-a-kubernetes-cluster-on-gke","title":"Step 1: Create a Kubernetes cluster on GKE","text":"<p>Run this command and all following commands on your local machine or on the Google Cloud Shell. If running from your local machine, you will need to install the Google Cloud SDK.</p> <pre><code>gcloud container clusters create aviary-gpu-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-standard-8\n</code></pre> <p>This command creates a Kubernetes cluster named <code>aviary-gpu-cluster</code> with 1 node in the <code>us-west1-b</code> zone. In this example, we use the <code>e2-standard-8</code> machine type, which has 8 vCPUs and 32 GB RAM. The cluster has autoscaling enabled, so the number of nodes can increase or decrease based on the workload.</p> <p>You can also create a cluster from the Google Cloud Console.</p>"},{"location":"kuberay/deploy-on-gke/#step-2-create-a-gpu-node-pool","title":"Step 2: Create a GPU node pool","text":"<p>Run the following command to create a GPU node pool for Ray GPU workers. (You can also create it from the Google Cloud Console; see the GKE documentation for more details.)</p> <pre><code>gcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4-vws,count=4 \\\n  --zone us-west1-b \\\n  --cluster aviary-gpu-cluster \\\n  --num-nodes 1 \\\n  --min-nodes 0 \\\n  --max-nodes 1 \\\n  --enable-autoscaling \\\n  --machine-type g2-standard-48 \\\n  --node-taints=ray.io/node-type=worker:NoSchedule \n</code></pre> <p>The <code>--accelerator</code> flag specifies the type and number of GPUs for each node in the node pool. In this example, we use the NVIDIA L4 GPU. The machine type <code>g2-standard-48</code> has 4 GPUs, 48 vCPUs and 192 GB RAM.</p> <p>Because this tutorial is for deploying 1 LLM, the maximum size of this GPU node pool is 1. If you want to deploy multiple LLMs in this cluster, you may need to increase the value of the max size.</p> <p>The taint <code>ray.io/node-type=worker:NoSchedule</code> prevents CPU-only Pods such as the Kuberay operator, Ray head, and CoreDNS pods from being scheduled on this GPU node pool. This is because GPUs are expensive, so we want to use this node pool for Ray GPU workers only.</p> <p>Concretely, any Pod that does not have the following toleration will not be scheduled on this GPU node pool:</p> <pre><code>tolerations:\n- key: ray.io/node-type\n  operator: Equal\n  value: worker\n  effect: NoSchedule\n</code></pre> <p>This toleration has already been added to the RayCluster YAML manifest <code>ray-cluster.aviary-gke.yaml</code> used in Step 6.</p> <p>For more on taints and tolerations, see the Kubernetes documentation.</p>"},{"location":"kuberay/deploy-on-gke/#step-3-configure-kubectl-to-connect-to-the-cluster","title":"Step 3: Configure <code>kubectl</code> to connect to the cluster","text":"<p>Run the following command to download credentials and configure the Kubernetes CLI to use them.</p> <pre><code>gcloud container clusters get-credentials aviary-gpu-cluster --zone us-west1-b\n</code></pre> <p>For more details, see the GKE documentation.</p>"},{"location":"kuberay/deploy-on-gke/#step-4-install-nvidia-gpu-device-drivers","title":"Step 4: Install NVIDIA GPU device drivers","text":"<p>This step is required for GPU support on GKE. See the GKE documentation for more details.</p> <pre><code># Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n\n# Verify that your nodes have allocatable GPUs \nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ...                                 4\n# ...                                 &lt;none&gt;\n</code></pre>"},{"location":"kuberay/deploy-on-gke/#step-5-install-the-kuberay-operator","title":"Step 5: Install the KubeRay operator","text":"<pre><code># Install both CRDs and KubeRay operator v0.5.0.\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0\n\n# It should be scheduled on the CPU node. If it is not, something is wrong.\n</code></pre>"},{"location":"kuberay/deploy-on-gke/#step-6-create-a-raycluster-with-aviary","title":"Step 6: Create a RayCluster with Aviary","text":"<p>If you are running this tutorial on the Google Cloud Shell, please copy the file <code>deploy/kuberay/ray-cluster.aviary-gke.yaml</code> to the Google Cloud Shell. You may find it useful to use the Cloud Shell Editor to edit the file.</p> <p>Now you can create a RayCluster with Aviary. Aviary is included in the image <code>anyscale/aviary:latest</code>, which is specified in the RayCluster YAML manifest <code>ray-cluster.aviary-gke.yaml</code>.</p> <pre><code># path: deploy/kuberay\nkubectl apply -f ray-cluster.aviary-gke.yaml\n</code></pre> <p>Note the following aspects of the YAML file:</p> <ul> <li> <p>The <code>tolerations</code> for workers match the taints we specified in Step 2. This ensures that the Ray GPU workers are scheduled on the GPU node pool.</p> <p>```yaml</p> </li> <li> <p>The field <code>rayStartParams.resources</code> has been configured to allow Ray to schedule Ray tasks and actors appropriately. The <code>mosaicml--mpt-7b-chat.yaml</code> file uses two custom resources, <code>accelerator_type_cpu</code> and <code>accelerator_type_a10</code>.  See the Ray documentation for more details on resources.</p> <p>```yaml</p> </li> </ul>"},{"location":"kuberay/deploy-on-gke/#please-add-the-following-taints-to-the-gpu-node","title":"Please add the following taints to the GPU node.","text":"<p>tolerations:     - key: \"ray.io/node-type\"     operator: \"Equal\"     value: \"worker\"     effect: \"NoSchedule\" ```</p>"},{"location":"kuberay/deploy-on-gke/#ray-head-the-ray-head-has-a-pod-resource-limit-of-2-cpus","title":"Ray head: The Ray head has a Pod resource limit of 2 CPUs.","text":"<p>rayStartParams:   resources: '\"{\\\"accelerator_type_cpu\\\": 2}\"'</p>"},{"location":"kuberay/deploy-on-gke/#ray-workers-the-ray-worker-has-a-pod-resource-limit-of-48-cpus-and-4-gpus","title":"Ray workers: The Ray worker has a Pod resource limit of 48 CPUs and 4 GPUs.","text":"<p>rayStartParams:     resources: '\"{\\\"accelerator_type_cpu\\\": 48, \\\"accelerator_type_a10\\\": 4}\"' ```</p>"},{"location":"kuberay/deploy-on-gke/#step-7-deploy-a-llm-model-with-aviary","title":"Step 7: Deploy a LLM model with Aviary","text":"<pre><code># Step 7.1: Log in to the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- bash\n\n# Step 7.2: Deploy the `mosaicml/mpt-7b-chat` model\naviary run --model ./models/static_batching/mosaicml--mpt-7b-chat.yaml\n\n# Step 7.3: Check the Serve application status\nserve status\n\n# [Example output]\n# name: default\n# app_status:\n#   status: RUNNING\n#   message: ''\n#   deployment_timestamp: 1686006910.9571936\n# deployment_statuses:\n# - name: default_mosaicml--mpt-7b-chat\n#   status: HEALTHY\n#   message: ''\n# - name: default_RouterDeployment\n#   status: HEALTHY\n#   message: ''\n\n# Step 7.4: List all models\nexport AVIARY_URL=\"http://localhost:8000\"\naviary models\n\n# [Example output]\n# Connecting to Aviary backend at:  http://localhost:8000/\n# mosaicml/mpt-7b-chat\n\n# Step 7.5: Send a query to `mosaicml/mpt-7b-chat`.\naviary query --model mosaicml/mpt-7b-chat --prompt \"What are the top 5 most popular programming languages?\"\n\n# [Example output]\n# Connecting to Aviary backend at:  http://localhost:8000/\n# mosaicml/mpt-7b-chat:\n# 1. Python\n# 2. Java\n# 3. JavaScript\n# 4. C++\n# 5. C#\n</code></pre>"},{"location":"kuberay/deploy-on-gke/#step-8-clean-up-resources","title":"Step 8: Clean up resources","text":"<p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p> <pre><code># Step 8.1: Delete the RayCluster\n# path: deploy/kuberay\nkubectl delete -f ray-cluster.aviary-gke.yaml\n\n# Step 8.2: Uninstall the KubeRay operator chart\nhelm uninstall kuberay-operator\n\n# Step 8.3: Delete the GKE cluster\ngcloud container clusters delete aviary-gpu-cluster\n</code></pre> <p>See the GKE documentation for more details on deleting a GKE cluster.</p>"}]}